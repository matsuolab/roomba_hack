<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 5 | ロボットシステム入門</title><link>https://matsuolab.github.io/roomba_hack/course/chap5/</link><atom:link href="https://matsuolab.github.io/roomba_hack/course/chap5/index.xml" rel="self" type="application/rss+xml"/><description>Chapter 5</description><generator>Wowchemy (https://wowchemy.com)</generator><language>ja</language><copyright>© 2022 Tokyo Robot And Intelligence Lab (TRAIL)</copyright><lastBuildDate>Sun, 09 Sep 2018 00:00:00 +0000</lastBuildDate><image><url>https://matsuolab.github.io/roomba_hack/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png</url><title>Chapter 5</title><link>https://matsuolab.github.io/roomba_hack/course/chap5/</link></image><item><title>三次元画像処理</title><link>https://matsuolab.github.io/roomba_hack/course/chap5/three-dimensions/</link><pubDate>Sat, 22 Jan 2022 00:00:00 +0000</pubDate><guid>https://matsuolab.github.io/roomba_hack/course/chap5/three-dimensions/</guid><description>&lt;h2 id="learn">Learn&lt;/h2>
&lt;p>今回はRealSenseD435というRGBDカメラを用いて三次元画像処理を行っていきましょう。&lt;/p>
&lt;h3 id="rgbdカメラについて">RGBDカメラについて&lt;/h3>
&lt;p>RGBDカメラとは、カラーの他にデプス(深度)を取得できるカメラのことです。
複雑な動作を行うロボットを動かす際には三次元空間の把握が重要となり、RGBDカメラはよく用いられます。
比較的安価でよく利用されるRGBDカメラとして、Intel社製のRealSenseやMicrosoft社製のXtionなどがあります。&lt;/p>
&lt;h3 id="realsense">RealSense&lt;/h3>
&lt;p>今回はRGBDカメラとしてRealSenseD435を使用します。&lt;/p>
&lt;p>ROSで用いる際には標準のラッパー(&lt;a href="https://github.com/IntelRealSense/realsense-ros">https://github.com/IntelRealSense/realsense-ros&lt;/a>)を使用します。&lt;/p>
&lt;p>&lt;code>roslaunch realsense2_camera rs_camera.launch&lt;/code>を行うとデフォルトのトピックとして
RGB画像の&lt;code>/camera/color/image_raw&lt;/code>、
デプス画像の&lt;code>/camera/depth/image_raw&lt;/code>
が利用できます。これらのトピックはいずれも&lt;code>sensor_msgs/Image&lt;/code>型です。&lt;/p>
&lt;p>RealSenseは物理的にRGB画像モジュールとデプス画像モジュールが離れているため、これら2つのトピックはいずれも画像データではあるものの、ピクセルの位置関係が対応しておらずそのままだとうまく画像処理に用いることができません。
そこで、起動時に&lt;code>align:=true&lt;/code>を指定することで、上記のトピックに加えてデプス画像をRGB画像のピクセルに対応するように変換する&lt;code>/camera/aligned_depth_to_color/image_raw&lt;/code>トピックを使用できるようにします。
他にも&lt;code>pointcloud:=true&lt;/code>を指定するとデプス画像から点群を生成することができます。
しかし、この処理は比較的重たいため今回はJetsonではなく、開発用PCでこの処理を行っていくことにします。&lt;/p>
&lt;p>それでは、RGB画像&lt;code>/camera/color/image_raw&lt;/code>と整列されたデプス画像&lt;code>/camera/aligned_depth_to_color/image_raw&lt;/code>の2種類のトピックを用いて三次元画像処理を行っていきましょう。&lt;/p>
&lt;h3 id="物体検出">物体検出&lt;/h3>
&lt;p>まずはRGB画像&lt;code>/camera/color/image_raw&lt;/code>のみを用いて三次元ではない画像検出を行っていきましょう。&lt;/p>
&lt;p>以下は&lt;code>/camera/color/image_raw&lt;/code>をSubscribeし、物体検出アルゴリズムであるYOLOv3に入力し、その結果をbounding boxとして描画し、&lt;code>/detection_result&lt;/code>としてPublishするスクリプトです。&lt;/p>
&lt;pre>&lt;code>#!/usr/bin/env python3
import rospy
from sensor_msgs.msg import Image
from cv_bridge import CvBridge
from pytorchyolo import detect, models
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import cv2
import copy
class ObjectDetection:
def __init__(self):
rospy.init_node('object_detection', anonymous=True)
# Publisher
self.detection_result_pub = rospy.Publisher('/detection_result', Image, queue_size=10)
# Subscriber
rgb_sub = rospy.Subscriber('/camera/color/image_raw', Image, self.callback_rgb)
self.bridge = CvBridge()
self.rgb_image = None
def callback_rgb(self, data):
cv_array = self.bridge.imgmsg_to_cv2(data, 'bgr8')
cv_array = cv2.cvtColor(cv_array, cv2.COLOR_BGR2RGB)
self.rgb_image = cv_array
def process(self):
path = &amp;quot;/root/roomba_hack/catkin_ws/src/three-dimensions_tutorial/yolov3/&amp;quot;
# load category
with open(path+&amp;quot;data/coco.names&amp;quot;) as f:
category = f.read().splitlines()
# prepare model
model = models.load_model(path+&amp;quot;config/yolov3.cfg&amp;quot;, path+&amp;quot;weights/yolov3.weights&amp;quot;)
while not rospy.is_shutdown():
if self.rgb_image is None:
continue
# inference
tmp_image = copy.copy(self.rgb_image)
boxes = detect.detect_image(model, tmp_image)
# [[x1, y1, x2, y2, confidence, class]]
# plot bouding box
for box in boxes:
x1, y1, x2, y2 = map(int, box[:4])
cls_pred = int(box[5])
tmp_image = cv2.rectangle(tmp_image, (x1, y1), (x2, y2), (0, 255, 0), 3)
tmp_image = cv2.putText(tmp_image, category[cls_pred], (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2)
# publish image
tmp_image = cv2.cvtColor(tmp_image, cv2.COLOR_RGB2BGR)
detection_result = self.bridge.cv2_to_imgmsg(tmp_image, &amp;quot;bgr8&amp;quot;)
self.detection_result_pub.publish(detection_result)
if __name__ == '__main__':
od = ObjectDetection()
try:
od.process()
except rospy.ROSInitException:
pass
&lt;/code>&lt;/pre>
&lt;p>コールバック関数で&lt;code>sensor_msgs/Image&lt;/code>型をnp.ndarray型に変換するために&lt;/p>
&lt;pre>&lt;code>cv_array = self.bridge.imgmsg_to_cv2(data, 'bgr8')
cv_array = cv2.cvtColor(cv_array, cv2.COLOR_BGR2RGB)
&lt;/code>&lt;/pre>
&lt;p>という&lt;code>sensor_msgs/Image&lt;/code>型特有の処理を行ってますが、Subscriberを作成しコールバック関数でデータを受け取るという基本的な処理の流れは&lt;code>scan&lt;/code>などの他のセンサと同じです。&lt;/p>
&lt;p>ここで注意してほしいのはYOLOの推論部分をコールバック関数内で行っていないことです。
一見、新しいデータが入ってくるときのみに推論を回すことは合理的に見えますが、センサの入力に対してコールバック関数内の処理が重いとセンサの入力がどんどん遅れていってしまいます。
コールバック関数内ではセンサデータの最低限の処理の記述にとどめ、重い処理は分けて書くことを意識しましょう。&lt;/p>
&lt;p>また、ここでは既存の物体検出モジュールを使用しましたが、PyTorchなどで作成した自作のモデルも同様の枠組みで利用することができます。&lt;/p>
&lt;p>続いて、整列されたデプス画像データも統合して物体を検出し、物体までの距離を測定してみましょう。&lt;/p>
&lt;p>RGB画像&lt;code>/camera/color/image_raw&lt;/code>と整列されたデプス画像&lt;code>/camera/aligned_depth_to_color/image_raw&lt;/code>はそれぞれ独立したトピックであるため、同期を取る必要があります。&lt;/p>
&lt;p>画像の同期にはmessage_filters(&lt;a href="http://wiki.ros.org/message_filters">http://wiki.ros.org/message_filters&lt;/a>)がよく使われます。&lt;/p>
&lt;p>message_filters.ApproximateTimeSynchronizerを使い以下のようにSubscriberを作成します。&lt;/p>
&lt;pre>&lt;code>rgb_sub = message_filters.Subscriber('/camera/color/image_raw', Image)
depth_sub = message_filters.Subscriber('/camera/aligned_depth_to_color/image_raw', Image)
message_filters.ApproximateTimeSynchronizer([rgb_sub, depth_sub], 10, 1.0).registerCallback(callback_rgbd)
def callback_rgbd(data1, data2):
bridge = CvBridge()
cv_array = bridge.imgmsg_to_cv2(data1, 'bgr8')
cv_array = cv2.cvtColor(cv_array, cv2.COLOR_BGR2RGB)
self.rgb_image = cv_array
cv_array = bridge.imgmsg_to_cv2(data2, 'passthrough')
self.depth_image = cv_array
&lt;/code>&lt;/pre>
&lt;p>この例では、1.0秒の許容で'/camera/color/image_raw&amp;rsquo;と'/camera/aligned_depth_to_color/image_raw&amp;rsquo;のトピックの同期を取ることができれば、コールバック関数callback_rgbdが呼ばれセンサデータが受けとられます。&lt;/p>
&lt;p>それでは、物体を検出し、物体までの距離を測定するスクリプトを見てみましょう。&lt;/p>
&lt;pre>&lt;code>#!/usr/bin/env python3
import rospy
import message_filters
from sensor_msgs.msg import Image
from cv_bridge import CvBridge
from pytorchyolo import detect, models
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import cv2
import copy
class DetectionDistance:
def __init__(self):
rospy.init_node('detection_distance', anonymous=True)
# Publisher
self.detection_result_pub = rospy.Publisher('/detection_result', Image, queue_size=10)
# Subscriber
rgb_sub = message_filters.Subscriber('/camera/color/image_raw', Image)
depth_sub = message_filters.Subscriber('/camera/aligned_depth_to_color/image_raw', Image)
message_filters.ApproximateTimeSynchronizer([rgb_sub, depth_sub], 10, 1.0).registerCallback(self.callback_rgbd)
self.bridge = CvBridge()
self.rgb_image, self.depth_image = None, None
def callback_rgbd(self, data1, data2):
cv_array = self.bridge.imgmsg_to_cv2(data1, 'bgr8')
cv_array = cv2.cvtColor(cv_array, cv2.COLOR_BGR2RGB)
self.rgb_image = cv_array
cv_array = self.bridge.imgmsg_to_cv2(data2, 'passthrough')
self.depth_image = cv_array
def process(self):
path = &amp;quot;/root/roomba_hack/catkin_ws/src/three-dimensions_tutorial/yolov3/&amp;quot;
# load category
with open(path+&amp;quot;data/coco.names&amp;quot;) as f:
category = f.read().splitlines()
# prepare model
model = models.load_model(path+&amp;quot;config/yolov3.cfg&amp;quot;, path+&amp;quot;weights/yolov3.weights&amp;quot;)
while not rospy.is_shutdown():
if self.rgb_image is None:
continue
# inference
tmp_image = copy.copy(self.rgb_image)
boxes = detect.detect_image(model, tmp_image)
# [[x1, y1, x2, y2, confidence, class]]
# plot bouding box
for box in boxes:
x1, y1, x2, y2 = map(int, box[:4])
cls_pred = int(box[5])
tmp_image = cv2.rectangle(tmp_image, (x1, y1), (x2, y2), (0, 255, 0), 3)
tmp_image = cv2.putText(tmp_image, category[cls_pred], (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2)
cx, cy = (x1+x2)//2, (y1+y2)//2
print(category[cls_pred], self.depth_image[cy][cx]/1000, &amp;quot;m&amp;quot;)
# publish image
tmp_image = cv2.cvtColor(tmp_image, cv2.COLOR_RGB2BGR)
detection_result = self.bridge.cv2_to_imgmsg(tmp_image, &amp;quot;bgr8&amp;quot;)
self.detection_result_pub.publish(detection_result)
if __name__ == '__main__':
dd = DetectionDistance()
try:
dd.process()
except rospy.ROSInitException:
&lt;/code>&lt;/pre>
&lt;p>基本的には物体検出のスクリプトと同じですが、&lt;/p>
&lt;pre>&lt;code>cx, cy = (x1+x2)//2, (y1+y2)//2
print(category[cls_pred], self.depth_image[cy][cx]/1000, &amp;quot;m&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>でbounding boxの中心座標を変換し、対応する距離をメートル単位で表示しています。&lt;/p>
&lt;p>整列されたデプス画像を用いているため、RGB画像に基づき算出した座標をそのまま指定できます。&lt;/p>
&lt;h3 id="点群の作成">点群の作成&lt;/h3>
&lt;p>上の例ではRGB画像とデプス画像を用いた三次元画像処理を行うことができました。&lt;/p>
&lt;p>しかし、ロボットの自立移動などより複雑な動作をさせることを考えたとき、深度データを三次元空間にマッピングできたほうが位置関係を統一的に扱うことができ便利なこともあります。&lt;/p>
&lt;p>それでデプス画像から点群と呼ばれるデータを作成することを考えます。&lt;/p>
&lt;p>点群とは三次元座標値(X,Y,Z)で構成された点の集まりのことです。各点の情報として、三次元座標値に加え色の情報(R,G,B)が加わることもあります。
デプス画像はカメラの内部パラメータを用いることによって点群データに変換することができます。(&lt;a href="https://medium.com/yodayoda/from-depth-map-to-point-cloud-7473721d3f">https://medium.com/yodayoda/from-depth-map-to-point-cloud-7473721d3f&lt;/a>)&lt;/p>
&lt;p>今回はdepth_image_procと呼ばれる、デプス画像を点群データに変換するROSの外部パッケージ(&lt;a href="http://wiki.ros.org/depth_image_proc">http://wiki.ros.org/depth_image_proc&lt;/a>) を使用して点群の変換を行います。&lt;/p>
&lt;p>外部パッケージは&lt;code>~/catkin_ws/src&lt;/code>等のワークスペースに配置し、ビルドしパスを通すことで簡単に使用できます。&lt;/p>
&lt;p>depth_image_procのwikiを参考に以下のようなlaunchファイルを作成しました。&lt;/p>
&lt;pre>&lt;code>&amp;lt;?xml version=&amp;quot;1.0&amp;quot;?&amp;gt;
&amp;lt;launch&amp;gt;
&amp;lt;node pkg=&amp;quot;nodelet&amp;quot; type=&amp;quot;nodelet&amp;quot; name=&amp;quot;nodelet_manager&amp;quot; args=&amp;quot;manager&amp;quot; /&amp;gt;
&amp;lt;node pkg=&amp;quot;nodelet&amp;quot; type=&amp;quot;nodelet&amp;quot; name=&amp;quot;nodelet1&amp;quot;
args=&amp;quot;load depth_image_proc/point_cloud_xyz nodelet_manager&amp;quot;&amp;gt;
&amp;lt;remap from=&amp;quot;camera_info&amp;quot; to=&amp;quot;/camera/color/camera_info&amp;quot;/&amp;gt;
&amp;lt;remap from=&amp;quot;image_rect&amp;quot; to=&amp;quot;/camera/aligned_depth_to_color/image_raw&amp;quot;/&amp;gt;
&amp;lt;remap from=&amp;quot;points&amp;quot; to=&amp;quot;/camera/depth/points&amp;quot;/&amp;gt;
&amp;lt;/node&amp;gt;
&amp;lt;/launch&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>このlaunchファイルを実行すると&lt;code>/camera/color/camera_info&lt;/code>と&lt;code>/camera/aligned_depth_to_color/image_raw&lt;/code>をSubscribeし、&lt;code>/camera/depth/points&lt;/code>をPublishします。&lt;/p>
&lt;p>&lt;code>/camera/color/camera_info&lt;/code>は&lt;code>sensor_msgs/CameraInfo&lt;/code>型のトピックであり、カメラパラメータやフレームid、タイムスタンプなどを保持しており、点群の変換に利用されます。
&lt;code>/camera/aligned_depth_to_color/image_raw&lt;/code>はRGB画像に整列されたデプス画像であるため、&lt;code>/camera/depth/camera_info&lt;/code>ではなく&lt;code>/camera/color/camera_info&lt;/code>を指定することに注意してください。&lt;/p>
&lt;p>&lt;code>roslaunch three-dimensions_tutorial depth2pc.launch&lt;/code>を行い&lt;code>/camera/depth/points&lt;/code>トピックをrvizで可視化をすると三次元空間に点群データが表示されているのが確認できます。&lt;/p>
&lt;h2 id="演習">演習&lt;/h2>
&lt;!-- &lt;details class="spoiler " id="spoiler-0">
&lt;summary>Dockerfileにamclを追加してBuildする&lt;/summary>
&lt;p>&lt;/p>
&lt;/details> -->
&lt;details class="spoiler " id="spoiler-1">
&lt;summary>ブランチの切り替え&lt;/summary>
&lt;p>&lt;pre>&lt;code>(jetson, 開発PC) git fetch
(jetson, 開発PC) git checkout feature/realsense
&lt;/code>&lt;/pre>
&lt;/p>
&lt;/details>
&lt;details class="spoiler " id="spoiler-2">
&lt;summary>(開発PC, jetson)起動準備&lt;/summary>
&lt;p>&lt;pre>&lt;code>(jetson)./RUN-DOCKER-CONTAINER.sh
(jetson)(docker) roslaunch roomba_bringup bringup.launch
(開発PC)./RUN-DOCKER-CONTAINER.sh 192.168.10.7x
&lt;/code>&lt;/pre>
&lt;p>&lt;code>roslaunch roomba_bringup bringup.launch&lt;/code>でRealSenseも同時に起動するようになりました。&lt;/p>
&lt;/p>
&lt;/details>
&lt;details class="spoiler " id="spoiler-3">
&lt;summary>(開発PC)RealSenseのトピックの可視化&lt;/summary>
&lt;p>&lt;pre>&lt;code>(開発PC)(docker) rviz
&lt;/code>&lt;/pre>
&lt;p>&lt;code>/camera/color/image_raw&lt;/code>と&lt;code>/camera/depth/image_raw&lt;/code>と&lt;code>/camera/aligned_depth_to_color/image_raw&lt;/code>を可視化して違いを確認してみよう。&lt;/p>
&lt;/p>
&lt;/details>
&lt;details class="spoiler " id="spoiler-4">
&lt;summary>(開発PC)物体検出を行う&lt;/summary>
&lt;p>&lt;pre>&lt;code>(開発PC)(docker) cd catkin_ws; catkin_make; source devel/setup.bash
(開発PC)(docker) roscd dimensions_tutorial; cd yolov3/weights; ./download_weights.sh
(開発PC)(docker) rosrun three-dimensions_tutorial object_detection.py
rvizで`/detection_result`を表示し結果を確認してみよう。
(開発PC)(docker) rosrun three-dimensions_tutorial detection_distance.py
&lt;/code>&lt;/pre>
&lt;/p>
&lt;/details>
&lt;details class="spoiler " id="spoiler-5">
&lt;summary>(開発PC)外部パッケージを使用&lt;/summary>
&lt;p>&lt;pre>&lt;code>(開発PC)(docker) cd ~/external_catkin_ws/src
(開発PC)(docker) git clone https://github.com/ros-perception/image_pipeline
(開発PC)(docker) cd ../; catkin build; source devel/setup.bash
(開発PC)(docker) roslaunch three-dimensions_tutorial depth2pc.launch
(開発PC)(docker) roslaunch navigation_tutorial navigation.launch
&lt;/code>&lt;/pre>
&lt;p>rvizで&lt;code>/camera/depth/points&lt;/code>トピックを追加して確認してみよう。&lt;/p>
&lt;/p>
&lt;/details></description></item></channel></rss>