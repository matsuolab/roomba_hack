<!doctype html><html lang=ja><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.2.0 for Hugo"><meta name=author content="TRAIL Admin"><meta name=description content><link rel=alternate hreflang=ja href=https://matsuolab.github.io/roomba_hack/course/chap5/three-dimensions/><link rel=preconnect href=https://fonts.gstatic.com crossorigin><meta name=theme-color content="#4caf50"><script src=/roomba_hack/js/mathjax-config.js></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload="this.media='all'"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload="this.media='all'" disabled><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload="this.media='all'"><link rel=stylesheet href=/roomba_hack/css/wowchemy.e6acd01addd9a75ba87b4d40035795b4.css><link rel=manifest href=/roomba_hack/index.webmanifest><link rel=icon type=image/png href=/roomba_hack/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_32x32_fill_lanczos_center_2.png><link rel=apple-touch-icon type=image/png href=/roomba_hack/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_180x180_fill_lanczos_center_2.png><link rel=canonical href=https://matsuolab.github.io/roomba_hack/course/chap5/three-dimensions/><meta property="twitter:card" content="summary"><meta property="og:site_name" content="ロボットシステム入門"><meta property="og:url" content="https://matsuolab.github.io/roomba_hack/course/chap5/three-dimensions/"><meta property="og:title" content="三次元画像処理 | ロボットシステム入門"><meta property="og:description" content><meta property="og:image" content="https://matsuolab.github.io/roomba_hack/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png"><meta property="twitter:image" content="https://matsuolab.github.io/roomba_hack/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png"><meta property="og:locale" content="ja"><meta property="article:published_time" content="2022-01-22T00:00:00+00:00"><meta property="article:modified_time" content="2022-01-22T00:00:00+00:00"><title>三次元画像処理 | ロボットシステム入門</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=6efadb7100ad083ad0bc43c5fd3c3b1a><script src=/roomba_hack/js/wowchemy-init.min.b8153d4570dcbb34350a2a846dba8c03.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/roomba_hack/>ロボットシステム入門</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/roomba_hack/>ロボットシステム入門</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class="nav-link active" href=/roomba_hack/course><span>Courses</span></a></li><li class=nav-item><a class=nav-link href=/roomba_hack/post><span>Blog</span></a></li><li class=nav-item><a class=nav-link href=/roomba_hack/contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></div><div class=page-body><div class="container-fluid docs"><div class="row flex-xl-nowrap"><div class="col-12 col-md-3 col-xl-2 docs-sidebar"><form class="docs-search d-flex align-items-center"><button class="btn docs-toggle d-md-none p-0 mr-md-3 w-100" type=button data-toggle=collapse data-target=#docs-nav aria-controls=docs-nav aria-expanded=false aria-label="Toggle section navigation"><div class=d-flex><span class="d-md-none pl-1 flex-grow-1 text-left overflow-hidden">Chapter 5</span>
<span><i class="fas fa-chevron-down"></i></span></div></button>
<button class="form-control sidebar-search js-search d-none d-md-flex">
<i class="fas fa-search pr-2"></i>
<span class=sidebar-search-text>Search...</span>
<span class=sidebar-search-shortcut>/</span></button></form><nav class="collapse docs-links" id=docs-nav><ul class="nav docs-sidenav"><li><a href=/roomba_hack/course/>ロボットシステム入門</a></li><div class=docs-toc-item><a class=docs-toc-link href=/roomba_hack/course/chap1/><i class="fas fa-book pr-1"></i>Chapter 1</a><ul class="nav docs-sidenav"><li><a href=/roomba_hack/course/chap1/%E9%96%8B%E7%99%BA%E7%92%B0%E5%A2%83/>開発環境</a></li><li><a href=/roomba_hack/course/chap1/ros/>ROSとは</a></li></ul></div><div class=docs-toc-item><a class=docs-toc-link href=/roomba_hack/course/chap2/><i class="fas fa-book pr-1"></i>Chapter 2</a><ul class="nav docs-sidenav"><li><a href=/roomba_hack/course/chap2/sensing1/>ロボットシステムにおけるセンシング・アクチュエーション・通信①</a></li></ul></div><div class=docs-toc-item><a class=docs-toc-link href=/roomba_hack/course/chap3/><i class="fas fa-book pr-1"></i>Chapter 3</a><ul class="nav docs-sidenav"><li><a href=/roomba_hack/course/chap3/sensing2/>ロボットシステムにおけるセンシング・アクチュエーション・通信②</a></li><li><a href=/roomba_hack/course/chap3/sensing3/>ロボットシステムにおけるセンシング・アクチュエーション・通信③</a></li></ul></div><div class=docs-toc-item><a class=docs-toc-link href=/roomba_hack/course/chap4/><i class="fas fa-book pr-1"></i>Chapter 4</a><ul class="nav docs-sidenav"><li><a href=/roomba_hack/course/chap4/localization/>自己位置推定</a></li><li><a href=/roomba_hack/course/chap4/navigation/>ナビゲーション</a></li></ul></div><div class=docs-toc-item><a class=docs-toc-link href=/roomba_hack/course/chap5/><i class="fas fa-book pr-1"></i>Chapter 5</a><ul class="nav docs-sidenav"><li class=active><a href=/roomba_hack/course/chap5/three-dimensions/>三次元画像処理</a></li></ul></div><div class=docs-toc-item><a class=docs-toc-link href=/roomba_hack/course/chap6/><i class="fas fa-book pr-1"></i>Chapter 6</a></div><div class=docs-toc-item><a class=docs-toc-link href=/roomba_hack/course/chap7/><i class="fas fa-book pr-1"></i>Chapter 7</a></div><div class=docs-toc-item><a class=docs-toc-link href=/roomba_hack/course/chap8/><i class="fas fa-book pr-1"></i>Chapter 8</a></div></ul></nav></div><div class="d-none d-xl-block col-xl-2 docs-toc"><ul class="nav toc-top"><li><a href=# id=back_to_top class=docs-toc-title>Contents</a></li></ul><nav id=TableOfContents><ul><li><a href=#learn>Learn</a><ul><li><a href=#rgbdカメラについて>RGBDカメラについて</a></li><li><a href=#realsense>RealSense</a></li><li><a href=#物体検出>物体検出</a></li><li><a href=#点群の作成>点群の作成</a></li></ul></li><li><a href=#演習>演習</a></li></ul></nav></div><main class="col-12 col-md-9 col-xl-8 py-md-3 pl-md-5 docs-content" role=main><article class=article><div class=docs-article-container><nav class="d-none d-md-flex" aria-label=breadcrumb><ol class=breadcrumb><li class=breadcrumb-item><a href=/roomba_hack/>Home</a></li><li class=breadcrumb-item><a href=/roomba_hack/course/>ロボットシステム入門</a></li><li class=breadcrumb-item><a href=/roomba_hack/course/chap5/>Chapter 5</a></li><li class="breadcrumb-item active" aria-current=page>三次元画像処理</li></ol></nav><h1>三次元画像処理</h1><div class=article-style><h2 id=learn>Learn</h2><p>今回はRealSenseD435というRGBDカメラを用いて三次元画像処理を行っていきましょう。</p><h3 id=rgbdカメラについて>RGBDカメラについて</h3><p>RGBDカメラとは、カラーの他にデプス(深度)を取得できるカメラのことです。
複雑な動作を行うロボットを動かす際には三次元空間の把握が重要となり、RGBDカメラはよく用いられます。
比較的安価でよく利用されるRGBDカメラとして、Intel社製のRealSenseやMicrosoft社製のXtionなどがあります。</p><h3 id=realsense>RealSense</h3><p>今回はRGBDカメラとしてRealSenseD435を使用します。</p><p>ROSで用いる際には標準のラッパー(<a href=https://github.com/IntelRealSense/realsense-ros>https://github.com/IntelRealSense/realsense-ros</a>)を使用します。</p><p><code>roslaunch realsense2_camera rs_camera.launch</code>を行うとデフォルトのトピックとして
RGB画像の<code>/camera/color/image_raw</code>、
デプス画像の<code>/camera/depth/image_raw</code>
が利用できます。これらのトピックはいずれも<code>sensor_msgs/Image</code>型です。</p><p>RealSenseは物理的にRGB画像モジュールとデプス画像モジュールが離れているため、これら2つのトピックはいずれも画像データではあるものの、ピクセルの位置関係が対応しておらずそのままだとうまく画像処理に用いることができません。
そこで、起動時に<code>align:=true</code>を指定することで、上記のトピックに加えてデプス画像をRGB画像のピクセルに対応するように変換する<code>/camera/aligned_depth_to_color/image_raw</code>トピックを使用できるようにします。
他にも<code>pointcloud:=true</code>を指定するとデプス画像から点群を生成することができます。
しかし、この処理は比較的重たいため今回はJetsonではなく、開発用PCでこの処理を行っていくことにします。</p><p>それでは、RGB画像<code>/camera/color/image_raw</code>と整列されたデプス画像<code>/camera/aligned_depth_to_color/image_raw</code>の2種類のトピックを用いて三次元画像処理を行っていきましょう。</p><h3 id=物体検出>物体検出</h3><p>まずはRGB画像<code>/camera/color/image_raw</code>のみを用いて三次元ではない画像検出を行っていきましょう。</p><p>以下は<code>/camera/color/image_raw</code>をSubscribeし、物体検出アルゴリズムであるYOLOv3に入力し、その結果をbounding boxとして描画し、<code>/detection_result</code>としてPublishするスクリプトです。</p><pre><code>#!/usr/bin/env python3

import rospy
from sensor_msgs.msg import Image
from cv_bridge import CvBridge
from pytorchyolo import detect, models
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import cv2
import copy

class ObjectDetection:
    def __init__(self):
        rospy.init_node('object_detection', anonymous=True)

        # Publisher
        self.detection_result_pub = rospy.Publisher('/detection_result', Image, queue_size=10)

        # Subscriber
        rgb_sub = rospy.Subscriber('/camera/color/image_raw', Image, self.callback_rgb)

        self.bridge = CvBridge()
        self.rgb_image = None

    def callback_rgb(self, data):
        cv_array = self.bridge.imgmsg_to_cv2(data, 'bgr8')
        cv_array = cv2.cvtColor(cv_array, cv2.COLOR_BGR2RGB)
        self.rgb_image = cv_array

    def process(self):
        path = &quot;/root/roomba_hack/catkin_ws/src/three-dimensions_tutorial/yolov3/&quot;

        # load category
        with open(path+&quot;data/coco.names&quot;) as f:
            category = f.read().splitlines()

        # prepare model
        model = models.load_model(path+&quot;config/yolov3.cfg&quot;, path+&quot;weights/yolov3.weights&quot;)

        while not rospy.is_shutdown():
            if self.rgb_image is None:
                continue

            # inference
            tmp_image = copy.copy(self.rgb_image)
            boxes = detect.detect_image(model, tmp_image)
            # [[x1, y1, x2, y2, confidence, class]]

            # plot bouding box
            for box in boxes:
                x1, y1, x2, y2 = map(int, box[:4])
                cls_pred = int(box[5])
                tmp_image = cv2.rectangle(tmp_image, (x1, y1), (x2, y2), (0, 255, 0), 3)
                tmp_image = cv2.putText(tmp_image, category[cls_pred], (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2)

            # publish image
            tmp_image = cv2.cvtColor(tmp_image, cv2.COLOR_RGB2BGR)
            detection_result = self.bridge.cv2_to_imgmsg(tmp_image, &quot;bgr8&quot;)
            self.detection_result_pub.publish(detection_result)


if __name__ == '__main__':
    od = ObjectDetection()
    try:
        od.process()
    except rospy.ROSInitException:
        pass
</code></pre><p>コールバック関数で<code>sensor_msgs/Image</code>型をnp.ndarray型に変換するために</p><pre><code>cv_array = self.bridge.imgmsg_to_cv2(data, 'bgr8')
cv_array = cv2.cvtColor(cv_array, cv2.COLOR_BGR2RGB)
</code></pre><p>という<code>sensor_msgs/Image</code>型特有の処理を行ってますが、Subscriberを作成しコールバック関数でデータを受け取るという基本的な処理の流れは<code>scan</code>などの他のセンサと同じです。</p><p>ここで注意してほしいのはYOLOの推論部分をコールバック関数内で行っていないことです。
一見、新しいデータが入ってくるときのみに推論を回すことは合理的に見えますが、センサの入力に対してコールバック関数内の処理が重いとセンサの入力がどんどん遅れていってしまいます。
コールバック関数内ではセンサデータの最低限の処理の記述にとどめ、重い処理は分けて書くことを意識しましょう。</p><p>また、ここでは既存の物体検出モジュールを使用しましたが、PyTorchなどで作成した自作のモデルも同様の枠組みで利用することができます。</p><p>続いて、整列されたデプス画像データも統合して物体を検出し、物体までの距離を測定してみましょう。</p><p>RGB画像<code>/camera/color/image_raw</code>と整列されたデプス画像<code>/camera/aligned_depth_to_color/image_raw</code>はそれぞれ独立したトピックであるため、同期を取る必要があります。</p><p>画像の同期にはmessage_filters(<a href=http://wiki.ros.org/message_filters>http://wiki.ros.org/message_filters</a>)がよく使われます。</p><p>message_filters.ApproximateTimeSynchronizerを使い以下のようにSubscriberを作成します。</p><pre><code>rgb_sub = message_filters.Subscriber('/camera/color/image_raw', Image)
depth_sub = message_filters.Subscriber('/camera/aligned_depth_to_color/image_raw', Image)
message_filters.ApproximateTimeSynchronizer([rgb_sub, depth_sub], 10, 1.0).registerCallback(callback_rgbd)

def callback_rgbd(data1, data2):
    bridge = CvBridge()
    cv_array = bridge.imgmsg_to_cv2(data1, 'bgr8')
    cv_array = cv2.cvtColor(cv_array, cv2.COLOR_BGR2RGB)
    self.rgb_image = cv_array

    cv_array = bridge.imgmsg_to_cv2(data2, 'passthrough')
    self.depth_image = cv_array
</code></pre><p>この例では、1.0秒の許容で'/camera/color/image_raw&rsquo;と'/camera/aligned_depth_to_color/image_raw&rsquo;のトピックの同期を取ることができれば、コールバック関数callback_rgbdが呼ばれセンサデータが受けとられます。</p><p>それでは、物体を検出し、物体までの距離を測定するスクリプトを見てみましょう。</p><pre><code>#!/usr/bin/env python3

import rospy
import message_filters
from sensor_msgs.msg import Image
from cv_bridge import CvBridge
from pytorchyolo import detect, models
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import cv2
import copy

class DetectionDistance:
    def __init__(self):
        rospy.init_node('detection_distance', anonymous=True)

        # Publisher
        self.detection_result_pub = rospy.Publisher('/detection_result', Image, queue_size=10)

        # Subscriber
        rgb_sub = message_filters.Subscriber('/camera/color/image_raw', Image)
        depth_sub = message_filters.Subscriber('/camera/aligned_depth_to_color/image_raw', Image)
        message_filters.ApproximateTimeSynchronizer([rgb_sub, depth_sub], 10, 1.0).registerCallback(self.callback_rgbd)

        self.bridge = CvBridge()
        self.rgb_image, self.depth_image = None, None

    def callback_rgbd(self, data1, data2):
        cv_array = self.bridge.imgmsg_to_cv2(data1, 'bgr8')
        cv_array = cv2.cvtColor(cv_array, cv2.COLOR_BGR2RGB)
        self.rgb_image = cv_array

        cv_array = self.bridge.imgmsg_to_cv2(data2, 'passthrough')
        self.depth_image = cv_array

    def process(self):
        path = &quot;/root/roomba_hack/catkin_ws/src/three-dimensions_tutorial/yolov3/&quot;

        # load category
        with open(path+&quot;data/coco.names&quot;) as f:
            category = f.read().splitlines()

        # prepare model
        model = models.load_model(path+&quot;config/yolov3.cfg&quot;, path+&quot;weights/yolov3.weights&quot;)

        while not rospy.is_shutdown():
            if self.rgb_image is None:
                continue

            # inference
            tmp_image = copy.copy(self.rgb_image)
            boxes = detect.detect_image(model, tmp_image)
            # [[x1, y1, x2, y2, confidence, class]]

            # plot bouding box
            for box in boxes:
                x1, y1, x2, y2 = map(int, box[:4])
                cls_pred = int(box[5])
                tmp_image = cv2.rectangle(tmp_image, (x1, y1), (x2, y2), (0, 255, 0), 3)
                tmp_image = cv2.putText(tmp_image, category[cls_pred], (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2)
                cx, cy = (x1+x2)//2, (y1+y2)//2
                print(category[cls_pred], self.depth_image[cy][cx]/1000, &quot;m&quot;)
            
            # publish image
            tmp_image = cv2.cvtColor(tmp_image, cv2.COLOR_RGB2BGR)
            detection_result = self.bridge.cv2_to_imgmsg(tmp_image, &quot;bgr8&quot;)
            self.detection_result_pub.publish(detection_result)


if __name__ == '__main__':
    dd = DetectionDistance()
    try:
        dd.process()
    except rospy.ROSInitException:
</code></pre><p>基本的には物体検出のスクリプトと同じですが、</p><pre><code>cx, cy = (x1+x2)//2, (y1+y2)//2
print(category[cls_pred], self.depth_image[cy][cx]/1000, &quot;m&quot;)
</code></pre><p>でbounding boxの中心座標を変換し、対応する距離をメートル単位で表示しています。</p><p>整列されたデプス画像を用いているため、RGB画像に基づき算出した座標をそのまま指定できます。</p><h3 id=点群の作成>点群の作成</h3><p>上の例ではRGB画像とデプス画像を用いた三次元画像処理を行うことができました。</p><p>しかし、ロボットの自立移動などより複雑な動作をさせることを考えたとき、深度データを三次元空間にマッピングできたほうが位置関係を統一的に扱うことができ便利なこともあります。</p><p>それでデプス画像から点群と呼ばれるデータを作成することを考えます。</p><p>点群とは三次元座標値(X,Y,Z)で構成された点の集まりのことです。各点の情報として、三次元座標値に加え色の情報(R,G,B)が加わることもあります。
デプス画像はカメラの内部パラメータを用いることによって点群データに変換することができます。(<a href=https://medium.com/yodayoda/from-depth-map-to-point-cloud-7473721d3f>https://medium.com/yodayoda/from-depth-map-to-point-cloud-7473721d3f</a>)</p><p>今回はdepth_image_procと呼ばれる、デプス画像を点群データに変換するROSの外部パッケージ(<a href=http://wiki.ros.org/depth_image_proc>http://wiki.ros.org/depth_image_proc</a>) を使用して点群の変換を行います。</p><p>外部パッケージは<code>~/catkin_ws/src</code>等のワークスペースに配置し、ビルドしパスを通すことで簡単に使用できます。</p><p>depth_image_procのwikiを参考に以下のようなlaunchファイルを作成しました。</p><pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;launch&gt;
  &lt;node pkg=&quot;nodelet&quot; type=&quot;nodelet&quot; name=&quot;nodelet_manager&quot; args=&quot;manager&quot; /&gt;

  &lt;node pkg=&quot;nodelet&quot; type=&quot;nodelet&quot; name=&quot;nodelet1&quot;
        args=&quot;load depth_image_proc/point_cloud_xyz nodelet_manager&quot;&gt;
    &lt;remap from=&quot;camera_info&quot; to=&quot;/camera/color/camera_info&quot;/&gt;
    &lt;remap from=&quot;image_rect&quot; to=&quot;/camera/aligned_depth_to_color/image_raw&quot;/&gt;
    &lt;remap from=&quot;points&quot; to=&quot;/camera/depth/points&quot;/&gt;
  &lt;/node&gt;
&lt;/launch&gt;
</code></pre><p>このlaunchファイルを実行すると<code>/camera/color/camera_info</code>と<code>/camera/aligned_depth_to_color/image_raw</code>をSubscribeし、<code>/camera/depth/points</code>をPublishします。</p><p><code>/camera/color/camera_info</code>は<code>sensor_msgs/CameraInfo</code>型のトピックであり、カメラパラメータやフレームid、タイムスタンプなどを保持しており、点群の変換に利用されます。
<code>/camera/aligned_depth_to_color/image_raw</code>はRGB画像に整列されたデプス画像であるため、<code>/camera/depth/camera_info</code>ではなく<code>/camera/color/camera_info</code>を指定することに注意してください。</p><p><code>roslaunch three-dimensions_tutorial depth2pc.launch</code>を行い<code>/camera/depth/points</code>トピックをrvizで可視化をすると三次元空間に点群データが表示されているのが確認できます。</p><h2 id=演習>演習</h2><details class=spoiler id=spoiler-1><summary>ブランチの切り替え</summary><p><pre><code>(jetson, 開発PC) git fetch
(jetson, 開発PC) git checkout feature/realsense
</code></pre></p></details><details class=spoiler id=spoiler-2><summary>(開発PC, jetson)起動準備</summary><p><pre><code>(jetson)./RUN-DOCKER-CONTAINER.sh
(jetson)(docker) roslaunch roomba_bringup bringup.launch
(開発PC)./RUN-DOCKER-CONTAINER.sh 192.168.10.7x
</code></pre><p><code>roslaunch roomba_bringup bringup.launch</code>でRealSenseも同時に起動するようになりました。</p></p></details><details class=spoiler id=spoiler-3><summary>(開発PC)RealSenseのトピックの可視化</summary><p><pre><code>(開発PC)(docker) rviz
</code></pre><p><code>/camera/color/image_raw</code>と<code>/camera/depth/image_raw</code>と<code>/camera/aligned_depth_to_color/image_raw</code>を可視化して違いを確認してみよう。</p></p></details><details class=spoiler id=spoiler-4><summary>(開発PC)物体検出を行う</summary><p><pre><code>(開発PC)(docker) cd catkin_ws; catkin_make; source devel/setup.bash
(開発PC)(docker) roscd dimensions_tutorial; cd yolov3/weights; ./download_weights.sh
(開発PC)(docker) rosrun three-dimensions_tutorial object_detection.py
rvizで`/detection_result`を表示し結果を確認してみよう。
(開発PC)(docker) rosrun three-dimensions_tutorial detection_distance.py
</code></pre></p></details><details class=spoiler id=spoiler-5><summary>(開発PC)外部パッケージを使用</summary><p><pre><code>(開発PC)(docker) cd ~/external_catkin_ws/src 
(開発PC)(docker) git clone https://github.com/ros-perception/image_pipeline
(開発PC)(docker) cd ../; catkin build; source devel/setup.bash
(開発PC)(docker) roslaunch three-dimensions_tutorial depth2pc.launch
(開発PC)(docker) roslaunch navigation_tutorial navigation.launch
</code></pre><p>rvizで<code>/camera/depth/points</code>トピックを追加して確認してみよう。</p></p></details><details class=spoiler id=spoiler-6><summary>余裕がある人向け</summary><p><p>物体を検出し、特定の物体の手前まで移動するスクリプトを作りましょう。</p><p>ヒント</p><ul><li>物体検出結果に基づいて物体部分以外をマスクしたデプス画像をpublishする</li><li>depth2pc.launchでそれをsubscribeし、point(cloud)に変換する</li><li>変換されたpointからmap座標系での位置を取得する</li><li>navigation_tutorial/scripts/set_goal.py (map座標系で指定した位置・姿勢までナビゲーションするスクリプト)などを参考に、その位置へとナビゲーションする</li></ul><p>PyTorchを使用した自作の分類器やネット上の分類器をシステムに組み込んでみましょう。</p><p>Lidarに映らない物体も画像ベースで検出しコストマップに追加することでナビゲーション時にぶつからないようにしましょう。</p></p></details></div><div class=article-widget><div class=post-nav></div></div></div><div class=body-footer><p>Last updated on 2022/1/22</p><p class=edit-page><a href=https://github.com/matsuolab/roomba_hack/edit/master/textbook/content/course/chap5/three-dimensions.md><i class="fas fa-pen pr-2"></i>Edit this page</a></p></div></article><footer class=site-footer><p class=powered-by>© 2022 Tokyo Robot And Intelligence Lab (TRAIL)</p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-modules target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></main></div></div></div><div class=page-footer></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.8.4/mermaid.min.js integrity="sha512-as1BF4+iHZ3BVO6LLDQ7zrbvTXM+c/1iZ1qII/c3c4L8Rn5tHLpFUtpaEtBNS92f+xGsCzsD7b62XP3XYap6oA==" crossorigin=anonymous title=mermaid></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/shell.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/bash.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/python.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/cpp.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/makefile.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.2.2/anchor.min.js integrity="sha512-I7w3ZdSFzw5j3jU3ZkNikBNeIrl3i+hEuEdwNmqUJvwNcaBUNcijnP2gd9DtGlgVYDplfjGoD8vTNsID+lCjqg==" crossorigin=anonymous></script><script>anchors.add()</script><script id=search-hit-fuse-template type=text/x-template>
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src=/roomba_hack/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js></script><script src=/roomba_hack/en/js/wowchemy.min.f2f4b99a7e752582c17f6e3437492b5f.js></script></body></html>